# -*- coding: utf-8 -*-
"""segmentation_deepnet_v3_slic_region_based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCc0SyQ6KQ5PFedsDXYlMXWm_P63UsB8
"""

import os
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, random_split, DataLoader, Subset
import torch.nn as nn
import torchvision.transforms.functional as TF
import random
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchvision

device = (
    "cuda"
    if torch.cuda.is_available()
    else "cpu"
)
print(f"Using {device} device")

class SegmentationOrientedDefectDataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=True):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.image_names = sorted(os.listdir(image_dir))
        self.mask_names = sorted(os.listdir(mask_dir))

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_names[idx])
        mask_path = os.path.join(self.mask_dir, self.mask_names[idx])

        image = Image.open(img_path).convert("RGB")
        mask = Image.open(mask_path).convert("L")  # grayscale (for binary)

        if self.transform:
            image, mask = self.augment(image, mask)

        # Normalize image and convert to tensor
        image = TF.to_tensor(image)
        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])

        # Convert mask to tensor and binarize
        mask = TF.to_tensor(mask)
        mask = (mask > 0.5).float()  # Ensure binary mask (0 or 1)

        return image, mask

    def augment(self, image, mask):
        # Random Horizontal Flip
        if random.random() > 0.5:
            image = TF.hflip(image)
            mask = TF.hflip(mask)

        # Then the Random Vertical Flip
        if random.random() > 0.5:
            image = TF.vflip(image)
            mask = TF.vflip(mask)

        # Random rotation
        angle = random.choice([0, 90, 180, 270])
        image = TF.rotate(image, angle)
        mask = TF.rotate(mask, angle)

        return image, mask

dataset_path = 'combined_dataset'

image_dir = os.path.join(dataset_path, "images")
mask_dir = os.path.join(dataset_path, "masks")

# dataset = SegmentationOrientedDefectDataset(image_dir, mask_dir, transform=True)
full_dataset = SegmentationOrientedDefectDataset(image_dir, mask_dir, transform=True)

random_indices = random.sample(range(len(full_dataset)), 5000)
dataset = Subset(full_dataset, random_indices)

# We will split it into train, val, test (e.g., 70%, 15%, 15%)
train_size = int(0.7*len(dataset))
val_size = int(0.15*len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

import torchvision
from torchvision.models.segmentation import deeplabv3_resnet50

def get_deeplabv3(num_classes=1):
    model = deeplabv3_resnet50(pretrained=True)

    # Replace classifier head
    model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=1)

    return model

model = get_deeplabv3()
print(model)

batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)

def dice_coefficient(preds, targets, threshold=0.5):
    preds = (torch.sigmoid(preds) > threshold).float()
    intersection = (preds * targets).sum()
    return (2. * intersection) / (preds.sum() + targets.sum() + 1e-8)

def iou_score(preds, targets, threshold=0.5):
    preds = (torch.sigmoid(preds) > threshold).float()
    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum() - intersection
    return intersection / (union + 1e-8)

from tqdm import tqdm

def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):
    best_val_dice = 0.0

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        train_dice = 0.0

        for images, masks in tqdm(train_loader, desc="Training", leave=False):
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(images)['out']
            loss = criterion(outputs, masks)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_dice += dice_coefficient(outputs, masks).item()

        train_loss = train_loss / len(train_loader)
        train_dice = train_dice / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0.0
        val_dice = 0.0
        val_iou = 0.0

        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc="Validating", leave=False):
                images = images.to(device)
                masks = masks.to(device)

                outputs = model(images)['out']
                loss = criterion(outputs, masks)

                val_loss += loss.item()
                val_dice += dice_coefficient(outputs, masks).item()
                val_iou += iou_score(outputs, masks).item()

        val_loss = val_loss / len(val_loader)
        val_dice = val_dice / len(val_loader)
        val_iou = val_iou / len(val_loader)

        print(f"Epoch [{epoch+1}/{epochs}]")
        print(f"Train Loss: {train_loss:.4f} | Dice: {train_dice:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Dice: {val_dice:.4f} | IoU: {val_iou:.4f}\n")

        # Save checkpoint
        if val_dice > best_val_dice:
            best_val_dice = val_dice
            torch.save(model.state_dict(), "model_saved")
            print(f"Saved new best model (Dice: {val_dice:.4f})")

model = get_deeplabv3().to(device)
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

train_model(model, train_loader, val_loader, optimizer, criterion, epochs=30)

torch.save(model.state_dict(), "deepnetv3.pth")

def visualize_prediction(image, mask, pred, alpha=0.6):
    """
    Visualizes the image, ground truth, predicted mask, and overlay of predicted mask on image.

    Parameters:
        image (Tensor): Original image tensor (C, H, W)
        mask (Tensor): Ground truth mask tensor (1, H, W)
        pred (Tensor): Predicted binary mask tensor (1, H, W)
        alpha (float): Transparency factor for overlay
    """
    image_np = image.cpu().permute(1, 2, 0).numpy()
    mask_np = mask.cpu().squeeze().numpy()
    pred_np = pred.cpu().squeeze().numpy()

    # Make overlay mask: green for predicted area
    overlay = image_np.copy()
    green_mask = np.zeros_like(image_np)
    green_mask[..., 1] = 1  # Green channel
    overlay_mask = np.where(pred_np[..., None] > 0, green_mask, 0)
    overlay = (1 - alpha) * image_np + alpha * overlay_mask
    overlay = np.clip(overlay, 0, 1)

    fig, axs = plt.subplots(1, 4, figsize=(16, 4))
    axs[0].imshow(image_np)
    axs[0].set_title('Original Image')
    axs[1].imshow(mask_np, cmap='gray')
    axs[1].set_title('Ground Truth Mask')
    axs[2].imshow(pred_np, cmap='gray')
    axs[2].set_title('Predicted Mask')
    axs[3].imshow(overlay)
    axs[3].set_title('Overlay on Image')

    for ax in axs:
        ax.axis('off')
    plt.tight_layout()
    plt.show()

def predict_and_evaluate(model, test_loader, save_dir, threshold=0.5, num_visualize=5):

    model.eval()
    dice_scores = []
    iou_scores = []

    with torch.no_grad():
        for idx, (images, masks) in enumerate(test_loader):
            images = images.to(device)
            masks = masks.to(device)

            # DeepLabv3 requires extracting 'out'
            outputs = model(images)['out']
            preds = torch.sigmoid(outputs)
            preds_bin = (preds > threshold).float()

            for i in range(images.size(0)):
                pred_mask = preds_bin[i].squeeze().cpu().numpy() * 255
                pred_mask_img = Image.fromarray(pred_mask.astype(np.uint8))
                pred_mask_img.save(os.path.join(save_dir, f"pred_{idx * test_loader.batch_size + i}.png"))

                # Evaluate against ground truth
                dice = dice_coefficient(preds_bin[i], masks[i])
                iou = iou_score(preds_bin[i], masks[i])
                dice_scores.append(dice.item())
                iou_scores.append(iou.item())

                if idx * test_loader.batch_size + i < num_visualize:
                    visualize_prediction(images[i], masks[i], preds_bin[i])

    print(f"\nAverage Dice Score on Test Set: {np.mean(dice_scores):.4f}")
    print(f"Average IoU Score on Test Set: {np.mean(iou_scores):.4f}")

model = get_deeplabv3()
model.load_state_dict(torch.load('deepnetv3.pth', map_location=device))
model.to(device)

model.eval()
predict_and_evaluate(model, test_loader, save_dir="DeepLabv3_predicted_masks", threshold=0.5)

"""#### **Superpixel-Based Refinement via Majority Voting**"""

from skimage.segmentation import slic
from skimage.color import label2rgb

def generate_superpixels(image, n_segments=300, compactness=10):
    # image must be in HWC format and float
    if image.dtype != np.float32:
        image = image.astype(np.float32) / 255.0

    superpixels = slic(image, n_segments=n_segments, compactness=compactness, start_label=0)
    return superpixels

from scipy import stats

def refine_with_superpixels(pred_mask, superpixels):
    refined_mask = np.zeros_like(pred_mask)

    for label in np.unique(superpixels):
        region_mask = (superpixels == label)
        majority_vote = stats.mode(pred_mask[region_mask].flatten(), keepdims=False).mode
        refined_mask[region_mask] = majority_vote

    return refined_mask

def evaluation_with_superpixels_refinement(model, test_loader, save_dir, threshold=0.5, num_visualize=5):
    os.makedirs(save_dir, exist_ok=True)
    model.eval()

    with torch.no_grad():
        for idx, (images, masks) in enumerate(test_loader):
            images = images.to(device)
            masks = masks.to(device)

            outputs = model(images)['out']
            preds = torch.sigmoid(outputs)
            preds_bin = (preds > threshold).float()

            for i in range(images.size(0)):
                image = images[i].cpu().permute(1, 2, 0).numpy()
                pred_mask = preds_bin[i].squeeze().cpu().numpy()

                # Now we will Generate Superpixels
                superpixels = generate_superpixels(image, n_segments=300, compactness=10)

                # Then we will apply the refinement
                refined_mask = refine_with_superpixels(pred_mask, superpixels)

                # Save the refined mask
                refined_img = Image.fromarray((refined_mask * 255).astype(np.uint8))
                refined_img.save(os.path.join(save_dir, f"refined_pred_{idx * test_loader.batch_size + i}.png"))

                if idx * test_loader.batch_size + i < num_visualize:
                    visualize_prediction(torch.tensor(image).permute(2, 0, 1), masks[i], torch.tensor(refined_mask))

save_dir = "DeepLabv3_refined_predicted_masks"
evaluation_with_superpixels_refinement(model, test_loader, save_dir, threshold=0.5)

"""#### **Region Splitting and Merging-Based Refinement**"""

import cv2
import numpy as np
from skimage.measure import label, regionprops
from scipy.ndimage import binary_fill_holes
from skimage.morphology import remove_small_objects, remove_small_holes

def region_based_split_and_merge(image, pred_mask, split_variance_thresh=0.01, merge_similarity_thresh=0.1, min_region_size=50):

    h, w = pred_mask.shape
    image_gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
    labeled_mask = label(pred_mask)

    region_map = np.zeros_like(pred_mask)

    # First is Region Splitting
    label_counter = 1
    for region in regionprops(labeled_mask):
        coords = region.coords
        region_intensity = image_gray[tuple(zip(*coords))]
        region_variance = np.var(region_intensity)

        if region_variance > split_variance_thresh:
            # split into 2 by k-means clustering
            pixels = np.array([image[coord[0], coord[1]] for coord in coords])
            pixels = np.float32(pixels)
            _, labels, _ = cv2.kmeans(pixels, 2, None,
                                      (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0),
                                      3, cv2.KMEANS_RANDOM_CENTERS)
            labels = labels.flatten()
            for i, coord in enumerate(coords):
                region_map[coord[0], coord[1]] = label_counter + labels[i]
            label_counter += 2
        else:
            for coord in coords:
                region_map[coord[0], coord[1]] = label_counter
            label_counter += 1

    # Step 4.3: Region Merging
    final_map = region_map.copy()
    merged = set()
    for label1 in np.unique(region_map):
        if label1 == 0 or label1 in merged:
            continue

        mask1 = (region_map == label1)
        mean1 = np.mean(image[mask1], axis=0)

        for label2 in np.unique(region_map):
            if label2 == 0 or label2 == label1 or label2 in merged:
                continue

            mask2 = (region_map == label2)
            mean2 = np.mean(image[mask2], axis=0)

            # Merge if similar color mean
            if np.linalg.norm(mean1 - mean2) < merge_similarity_thresh:
                final_map[mask2] = label1
                merged.add(label2)

    # Step 4.4: Optional Morphological Cleaning
    binary_mask = final_map > 0
    binary_mask = binary_fill_holes(binary_mask)
    binary_mask = remove_small_objects(binary_mask, min_size=min_region_size)
    binary_mask = remove_small_holes(binary_mask, area_threshold=min_region_size)

    # Step 4.5: Final Output
    return binary_mask.astype(np.uint8)

def apply_split_merge_refinement(model, test_loader, save_dir, threshold=0.5, num_visualize=5):
    os.makedirs(save_dir, exist_ok=True)
    model.eval()

    with torch.no_grad():
        for idx, (images, masks) in enumerate(test_loader):
            images = images.to(device)
            outputs = model(images)['out']
            preds = torch.sigmoid(outputs)
            preds_bin = (preds > threshold).float()

            for i in range(images.size(0)):
                image = images[i].cpu().permute(1, 2, 0).numpy()
                pred_mask = preds_bin[i].squeeze().cpu().numpy()

                # Apply Region Splitting and Merging Refinement
                refined_mask = region_based_split_and_merge(image, pred_mask)

                # Save
                refined_img = Image.fromarray((refined_mask * 255).astype(np.uint8))
                refined_img.save(os.path.join(save_dir, f"split_merge_refined_{idx * test_loader.batch_size + i}.png"))

                if idx * test_loader.batch_size + i < num_visualize:
                    visualize_prediction(torch.tensor(image).permute(2, 0, 1), masks[i], torch.tensor(refined_mask))

    print("Region splitting & merging-based refinement complete.")

save_dir = "Region_Based"
apply_split_merge_refinement(model, test_loader, save_dir, threshold=0.5)

def visualize_full_pipeline(image, mask, pred_mask, slic_refined, region_refined):
    """
    Display the original image, ground truth, predicted mask,
    slic-refined mask, and region-based refined mask side by side.
    """
    image = image.cpu().permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)
    mask = mask.cpu().squeeze().numpy()
    pred_mask = pred_mask.squeeze()
    slic_refined = slic_refined.squeeze()
    region_refined = region_refined.squeeze()

    fig, axs = plt.subplots(1, 5, figsize=(20, 4))

    axs[0].imshow(image)
    axs[0].set_title("Original Image")

    axs[1].imshow(mask, cmap='gray')
    axs[1].set_title("Ground Truth")

    axs[2].imshow(pred_mask, cmap='gray')
    axs[2].set_title("Predicted Mask")

    axs[3].imshow(slic_refined, cmap='gray')
    axs[3].set_title("SLIC Refined")

    axs[4].imshow(region_refined, cmap='gray')
    axs[4].set_title("Region-Based Refined")

    for ax in axs:
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# Visualization for a few test images
num_visualize = 5
model.eval()

with torch.no_grad():
    for idx in range(num_visualize):
        image, mask = test_dataset[idx]
        image_input = image.unsqueeze(0).to(device)

        output = model(image_input)['out']
        pred = torch.sigmoid(output).squeeze().cpu().numpy()
        pred_bin = (pred > 0.5).astype(np.uint8)

        image_np = image.permute(1, 2, 0).numpy()

        # Apply Superpixel refinement
        superpixels = generate_superpixels(image_np)
        slic_refined = refine_with_superpixels(pred_bin, superpixels)

        # Apply Region Splitting & Merging refinement
        region_refined = region_based_split_and_merge(image_np, pred_bin)

        # Show all in one plot
        visualize_full_pipeline(image, mask, pred_bin, slic_refined, region_refined)